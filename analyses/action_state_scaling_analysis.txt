================================================================================
ACTION VE STATE SCALING/NORMALIZATION ANALİZİ
================================================================================

SORUN: Agent hep aynı tarafa açı vererek yükseliyor (OutOfBounds)

Episode 1-10 Gözlemleri:
- Tüm episode'lar OutOfBounds
- End positions: 25-40m (limit 25m)
- End velocities: 2.5-9.6 m/s (yüksek, pozitif!)
- Hep aynı yöne gidiyor gibi görünüyor

================================================================================
1. ACTION SPACE KONTROLÜ:
================================================================================

AGENT.PY:
- Action output: tanh() → [-1, 1] aralığı
- [pitch, yaw, thrust_raw, roll]
- log_std = -1.0 (başlangıç)
  - std = exp(-1.0) ≈ 0.37
  - Bu ÇOK DÜŞÜK exploration!

ENV.PY STEP():
- pitch = action[0] (direkt [-1, 1])
- yaw = action[1] (direkt [-1, 1])
- thrust = 0.5 * (action[2] + 1.0) → [0, 1]
- roll = action[3] (direkt [-1, 1])

UNITY ENV.CS:
- ApplyPhysics() metodunda pitch/yaw kullanımı?

SORUN TESPİTİ:
1. log_std = -1.0 → exploration çok düşük
2. Action space simetrik görünüyor ama exploration yetersiz
3. Initial entropy çok düşük olabilir

================================================================================
2. STATE NORMALIZATION KONTROLÜ:
================================================================================

DX/DZ NORMALIZATION:
```python
states_norm[0] = np.clip(states[0] / self.dx_scale, -1.0, 1.0)  # dx
states_norm[2] = np.clip(states[2] / self.dx_scale, -1.0, 1.0)  # dz
```

- dx_scale = 45.0
- Simetrik normalize ediliyor (clip -1, 1)
- GÖRÜNÜŞTE DOĞRU

SORUN: dx ve dz için eşit muamele var ama...
- Loglardan görünen: End positions hep pozitif tarafta (25-40m)
- Bu bias olabilir mi?

QUATERNION NORMALIZATION:
- states_norm[9:13] = states[9:13] (dokunulmuyor)
- Unity'den gelen quaternion normalize mi?

================================================================================
3. REWARD FUNCTION SİMETRİ KONTROLÜ:
================================================================================

DISTANCE CALCULATION:
```python
dist_h = (dx*dx + dz*dz) ** 0.5
```

- Simetrik (dx ve dz eşit ağırlıkta)
- GÖRÜNÜŞTE DOĞRU

HORIZONTAL VELOCITY:
```python
v_h = (vx*vx + vz*vz) ** 0.5
```

- Simetrik (vx ve vz eşit ağırlıkta)
- GÖRÜNÜŞTE DOĞRU

SORUN: Reward function simetrik görünüyor ama...
- Agent hep aynı tarafa gidiyorsa, bir bias var olabilir
- Initial conditions bias'ı?
- Unity'de bir bias?

================================================================================
4. INITIAL CONDITIONS KONTROLÜ:
================================================================================

ENV.PY INITIALSTART():
```python
x = np.random.uniform(self.init_x_min, self.init_x_max)  # -5.0 to 5.0
z = np.random.uniform(self.init_z_min, self.init_z_max)  # -5.0 to 5.0
pitch = np.random.uniform(self.init_pitch_min, self.init_pitch_max)  # -2.0 to 2.0
yaw = np.random.uniform(self.init_yaw_min, self.init_yaw_max)  # -2.0 to 2.0
```

- Simetrik aralıklar
- GÖRÜNÜŞTE DOĞRU

================================================================================
5. KRİTİK SORUN: EXPLORATION YETERSİZLİĞİ
================================================================================

AGENT.PY:
```python
self.log_std = tf.Variable(-1.0*tf.ones((self.action_size), dtype=tf.float32),
                           trainable=True,
                           name="log_std")
```

- log_std = -1.0
- std = exp(-1.0) ≈ 0.37
- Bu ÇOK KÜÇÜK!

ETKİ:
- Agent çok az exploration yapıyor
- Deterministic policy'ye yakın davranıyor
- Aynı action'ları tekrarlıyor
- Exploration yetersiz → hep aynı tarafa gidiyor

ÇÖZÜM:
- log_std başlangıç değeri artırılmalı
- Öneri: -0.5 veya 0.0
- std = exp(-0.5) ≈ 0.61 veya exp(0.0) = 1.0

================================================================================
6. DİĞER OLASI SORUNLAR:
================================================================================

A. UNITY BIAS:
- Unity'de RCS veya physics'te bir bias olabilir
- ApplyPhysics() metodunu kontrol et

B. ACTION CLIPPING:
- Action [-1, 1] aralığında ama Unity'de nasıl kullanılıyor?
- Clipping yanlış yapılıyor olabilir

C. STATE BIAS:
- Quaternion normalization sorunu?
- Unity'den gelen state'ler bias'lı?

================================================================================
7. ÖNERİLEN DÜZELTMELER:
================================================================================

1. EN ÖNEMLİ: log_std Başlangıç Değerini Artır
   ```python
   self.log_std = tf.Variable(0.0*tf.ones(...))  # -1.0 → 0.0
   ```
   - std = 1.0 → daha fazla exploration
   - Agent farklı action'lar deneyecek

2. Entropy Coefficient Kontrolü
   ```python
   self.ent_coef = 0.01  # Şu anki
   ```
   - Bu yeterli mi? Test et
   - Belki 0.02-0.05 olabilir

3. Action Space Symmetry Test
   - Unity'de pitch/yaw'ın simetrik çalıştığını doğrula
   - Test: +pitch ve -pitch aynı etkiyi yaratıyor mu?

4. Initial Conditions Range Artırma
   - Belki initial pitch/yaw range'i artırılabilir
   - -2.0 to 2.0 → -5.0 to 5.0?

================================================================================
8. ACİL ÇÖZÜM:
================================================================================

log_std'yi artır:
- -1.0 → 0.0 (2.7x daha fazla exploration)
- Veya -1.0 → -0.5 (1.6x daha fazla exploration)

Bu değişiklik hemen etki edecek:
- Agent daha fazla farklı action'lar deneyecek
- Symmetry sorunu çözülecek (exploration yeterli olursa)
- OutOfBounds sorunu azalacak (daha çeşitli davranış)

================================================================================

