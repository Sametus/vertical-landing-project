================================================================================
CEILING REWARD HACKING ANALİZİ
================================================================================

SORU: Roket yukarı doğru hızlandığında irtifa kazanacak ve aldığı ceza azalacak.
      Bu ceiling'e teşvik etmiyor mu?

CEVAP: ✅ EVET, TEŞVİK EDİYOR! (Reward Hacking Riski)

================================================================================
1. SORUN: REWARD HACKING
================================================================================

SENARYO: Roket yukarı gider (vy=2 m/s), irtifa kazanır, sonra aşağı düşer (vy=-2 m/s)

dy (m) | Yukarı ceza | Yükseklik ceza | Aşağı ceza | TOPLAM CEZA
-------|-------------|----------------|-------------|-------------
20.0   | 0.267       | 0.000          | 0.274       | 0.541
30.0   | 0.400       | 0.000          | 0.237       | 0.637
40.0   | 0.533       | 0.000          | 0.219       | 0.752
50.0   | 0.667       | 0.000          | 0.207       | 0.874

GÖZLEM:
⚠️ İrtifa arttıkça TOPLAM CEZA artıyor (0.541 → 0.874)
⚠️ AMA aşağı düşüş cezası AZALIYOR (0.274 → 0.207)!
⚠️ Agent yukarı çıkarsa → İrtifa kazanır → Sonra aşağı düşünce daha AZ ceza alır!

MEVCUT CEZALAR:

1. YUKARI GİTME CEZASI:
   ```python
   up_penalty = 0.20 * vy * (dy / 30.0)
   ```
   - dy=30m, vy=2 m/s → 0.4 ceza (scaling sonrası: 0.2)
   - dy=50m, vy=2 m/s → 0.67 ceza (scaling sonrası: 0.335)
   ⚠️ Çok küçük!

2. YÜKSEKLİK CEZASI:
   ```python
   if dy > 20.0:
       height_penalty = -0.04 * (dy - 20.0)
   ```
   - dy=30m → -0.4 ceza (scaling sonrası: -0.2)
   - dy=50m → -1.2 ceza (scaling sonrası: -0.6)
   ⚠️ Küçük ve sadece dy > 20.0 için!

3. AŞAĞI DÜŞÜŞ CEZASI (PROGRESSIVE):
   ```python
   altitude_factor = 1.0 + (15.0 / (dy + 1.0))
   reward -= 0.08 * abs(vy) * altitude_factor
   ```
   - dy=30m → factor=1.48x, vy=-2 m/s → 0.237 ceza
   - dy=50m → factor=1.29x, vy=-2 m/s → 0.207 ceza
   ⚠️ İrtifa arttıkça AZALIYOR!

4. CEILING HIT:
   ```python
   if dy >= 60.0 and vy > 0.3:
       return -1000.0, True  # Scaling sonrası: -500
   ```
   ✅ Var ama çok geç (60m)!

================================================================================
2. REWARD HACKING MEKANİZMASI
================================================================================

AGENT'IN ÖĞRENEBİLECEĞİ STRATEJİ:

1. Yukarı çık (vy > 0) → İrtifa kazan (dy artar)
   - Yukarı gitme cezası: 0.4-0.67 (küçük!)
   - Yükseklik cezası: -0.4 to -1.2 (küçük!)
   - NET: Küçük ceza veya küçük ödül

2. Sonra aşağı düş (vy < 0) → Artık dy yüksek olduğu için ceza AZ!
   - dy=50m'de aşağı düşüş cezası: 0.207
   - dy=20m'de aşağı düşüş cezası: 0.274
   - FARK: 0.067 (agent için avantaj!)

3. Yüksek irtifada daha fazla manevra alanı → Daha fazla time step → Daha fazla reward!

SONUÇ:
⚠️ Agent yukarı çıkmayı öğrenebilir (reward hacking)
⚠️ Ceiling'e (50-59m) yaklaşabilir
⚠️ CeilingHit (60m) çok geç, agent oraya ulaşmadan önce yukarı çıkabilir

================================================================================
3. ÇÖZÜM ÖNERİLERİ
================================================================================

A) YUKARI GİTME CEZASINI ARTIR (ÖNERİLEN):
   
   MEVCUT: `up_penalty = 0.20 * vy * (dy / 30.0)`
   
   ÖNERİ 1: `up_penalty = 0.40 * vy * (dy / 20.0)`  # 2x artırıldı
   - dy=30m, vy=2 m/s → 1.2 ceza (önceden 0.4)
   - dy=50m, vy=2 m/s → 2.0 ceza (önceden 0.67)
   
   ÖNERİ 2: `up_penalty = 0.30 * vy * np.exp(dy / 30.0)`  # Exponential
   - dy=30m, vy=2 m/s → 1.63 ceza
   - dy=50m, vy=2 m/s → 2.99 ceza

B) YÜKSEKLİK CEZASINI GÜÇLENDİR (ÖNERİLEN):
   
   MEVCUT: `if dy > 20.0: height_penalty = -0.04 * (dy - 20.0)`
   
   ÖNERİ: `if dy > 15.0: height_penalty = -0.08 * (dy - 15.0)`  # 2x artırıldı, threshold düşürüldü
   - dy=30m → -1.2 ceza (önceden -0.4)
   - dy=50m → -2.8 ceza (önceden -1.2)

C) CEILING THRESHOLD'U DÜŞÜR:
   
   MEVCUT: `if dy >= 60.0`
   
   ÖNERİ: `if dy >= 50.0`  # Daha erken yakala
   - Agent 50m'ye ulaşınca hemen bitir

D) YÜKSEK İRTİFADA EK CEZA (ÖNERİLEN):
   
   ```python
   # Yüksek irtifada ek progressive ceza (ceiling'e yaklaşmayı caydır)
   if dy > 30.0:
       high_altitude_penalty = -0.05 * (dy - 30.0)  # 30m üzeri için ek ceza
       reward += high_altitude_penalty
   ```
   - dy=40m → -0.5 ceza
   - dy=50m → -1.0 ceza

================================================================================
4. ÖNERİLEN ÇÖZÜM KOMBİNASYONU
================================================================================

```python
# YUKARI GİTME CEZASI (artırıldı)
if vy > 0.0:
    up_penalty = 0.40 * vy * (dy / 20.0)  # 2x artırıldı: 0.20 → 0.40
    reward -= up_penalty

# YÜKSEKLİK CEZASI (güçlendirildi)
if dy > 15.0:
    height_penalty = -0.08 * (dy - 15.0)  # 2x artırıldı, threshold düşürüldü
    reward += height_penalty

# YÜKSEK İRTİFADA EK CEZA (ceiling'e yaklaşmayı caydır)
if dy > 30.0:
    high_altitude_penalty = -0.05 * (dy - 30.0)
    reward += high_altitude_penalty

# CEILING THRESHOLD (düşürüldü)
if dy >= 50.0 and vy > 0.3:  # 60.0 → 50.0
    self.termination_reason = "CeilingHit"
    return -1000.0, True
```

ETKİ:
- dy=30m, vy=2 m/s yukarı → Toplam ceza: ~1.8 (önceden ~0.6)
- dy=50m, vy=2 m/s yukarı → Toplam ceza: ~4.2 (önceden ~1.9)
- Ceiling'e yaklaşma çok daha cezalandırıcı!

================================================================================
5. SONUÇ
================================================================================

SORUN:
✅ Agent yukarı çıkarak "ceza azaltma" yapabilir (reward hacking)
✅ Ceiling'e (50-59m) yaklaşabilir
✅ Mevcut cezalar yetersiz

ÇÖZÜM:
1. ✅ Yukarı gitme cezasını artır (0.20 → 0.40)
2. ✅ Yükseklik cezasını güçlendir (0.04 → 0.08, threshold 20→15)
3. ✅ Yüksek irtifada ek ceza ekle (dy > 30.0 için)
4. ✅ Ceiling threshold'u düşür (60 → 50)

GEREKÇE:
✅ Ceiling'e yaklaşmayı caydırır
✅ Reward hacking'i önler
✅ Agent yukarı çıkmayı öğrenmez

================================================================================

