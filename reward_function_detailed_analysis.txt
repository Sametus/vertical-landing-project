================================================================================
REWARD FUNCTION DETAYLI ANALİZİ: Rocket İniş Problemi
================================================================================

MEVCUT DURUM ANALİZİ:
================================================================================

1. TERMINAL REWARDS (Episode sonu):
   - Success: +1500 + time_bonus (max ~+1580)
   - Crash: -300
   - MissedZone: -350 (Crash'ten daha kötü!)
   - OutOfBounds/Tilted/Spin/Ceiling: -500
   - TimeLimit: shaping_reward - 60

2. SHAPING REWARDS (Havada, her step):
   - Step penalty: -0.02 (yaşam maliyeti)
   - Distance penalty: -0.05 * dist_h (yatay mesafe cezası)
   - Horizontal velocity: -0.03 * v_h
   - Vertical velocity (near ground): -0.05 * abs(vy) / (dy + 1.0) [ONLY if dy < 15 AND vy < 0]
   - Tilt penalty: -0.10 * (0.85 - up_y) [if up_y < 0.85]
   - Tilt bonus: +0.04 * up_y [if up_y >= 0.85]
   - Spin penalty: -0.01 * w_mag

3. REWARD SCALING: 0.1x (tüm rewardlar 10'a bölünüyor)

================================================================================
KRİTİK SORUNLAR:
================================================================================

1. HIZ KONTROLÜ YETERSİZ (EN BÜYÜK SORUN!)
   Problem:
   - Vertical velocity penalty sadece dy < 15m ve vy < 0 iken aktif
   - Yüksek irtifada (dy > 15m) hız kontrolü YOK!
   - Agent serbest düşüş yapıp son anda fren yapmaya çalışıyor

   Mevcut kod:
   ```python
   if vy < 0.0 and dy < 15.0:
       reward += -0.05 * abs(vy) / (dy + 1.0)
   ```

   Sorun:
   - 20m'de -10 m/s düşüyorsa → ceza yok!
   - 10m'de -5 m/s düşüyorsa → ceza var (-0.05 * 5 / 11 = -0.023)
   - Cezalar çok küçük (max -0.05 * 10 / 1 = -0.5, scaling sonrası -0.05)

2. REWARD SCALING ÇOK AGGRESSIVE
   Problem:
   - Tüm rewardlar 0.1x
   - Success: +150 → +15 (çok küçük!)
   - Shaping rewards: max ±0.04 → ±0.004 (neredeyse sıfır!)

   Etki:
   - Agent shaping signal'ları göremiyor
   - Sadece terminal reward'lar önemli (sparse reward gibi)
   - Exploration çok zor

3. DISTANCE PENALTY ÇOK DOMINANT
   Problem:
   - -0.05 * dist_h
   - 10m distance = -0.5 (scaling öncesi), -0.05 (scaling sonrası)
   - Bu shaping reward'lardan çok daha büyük
   - Agent sadece "merkeze git" öğreniyor, "yavaş in" öğrenemiyor

4. HORIZONTAL VELOCITY PENALTY YETERSİZ
   Problem:
   - -0.03 * v_h
   - 5 m/s hız = -0.15 (scaling öncesi), -0.015 (scaling sonrası)
   - Çok küçük! Agent drift edebiliyor

5. SUCCESS CRITERIA GEVŞEK
   Problem:
   - vy <= 4.5 m/s (çok gevşek!)
   - v_h <= 3.0 m/s
   - dist_h <= 4.0m
   - Agent bunu bile tutturamıyor, daha da gevşetmek mantıksız

6. STEP PENALTY ÇOK KÜÇÜK
   Problem:
   - -0.02 (scaling öncesi), -0.002 (scaling sonrası)
   - Agent "uzun süre havada kal" öğreniyor (hız kontrolü yapmadan)

7. TILT PENALTY/BONUS DENGESİZLİK
   Problem:
   - Penalty: -0.10 * (0.85 - up_y) [max -0.085]
   - Bonus: +0.04 * up_y [max +0.034]
   - Penalty > Bonus (asimmetrik)

================================================================================
MATEMATİKSEL ANALİZ:
================================================================================

Örnek Senaryo: Agent 20m yükseklikte, -8 m/s ile düşüyor

Mevcut reward (scaling öncesi):
- Step: -0.02
- Distance (5m): -0.25
- Horizontal vel (1 m/s): -0.03
- Vertical vel: 0 (dy > 15, ceza yok!)
- Tilt (0.9): +0.036
- Spin (0.5 rad/s): -0.005
TOTAL: -0.279 (scaling sonrası: -0.0279)

Sorun: Hız kontrolü sinyali YOK! Agent "serbest düşüş" yapıyor.

Örnek Senaryo: Agent 10m yükseklikte, -5 m/s ile düşüyor

Mevcut reward (scaling öncesi):
- Step: -0.02
- Distance (5m): -0.25
- Horizontal vel (1 m/s): -0.03
- Vertical vel: -0.05 * 5 / 11 = -0.023
- Tilt (0.9): +0.036
- Spin (0.5 rad/s): -0.005
TOTAL: -0.302 (scaling sonrası: -0.0302)

Sorun: Hız cezası çok küçük! (-0.023 vs distance -0.25)

================================================================================
ROKET İNİŞ STRATEJİSİ:
================================================================================

İdeal iniş profili:
1. Yüksek irtifa (20-15m): Yavaşça alçal, yatay drift'i düzelt
2. Orta irtifa (15-5m): Yatay hızı sıfırla, dikey hızı yavaşlat
3. Düşük irtifa (5-1.5m): Çok yavaş alçal (vy ≈ -1 ila -2 m/s)
4. İniş (dy < 1.5m): Yumuşak iniş (vy > -2.5 m/s)

Mevcut reward function bunu teşvik etmiyor!

================================================================================
ÖNERİLEN DÜZELTMELER:
================================================================================

1. HIZ KONTROLÜ: HER İRTİFADA AKTİF
   - Vertical velocity penalty'i tüm irtifalar için aktif yap
   - İrtifa azaldıkça artan penalty (progressive)
   - Örnek: reward -= 0.10 * abs(vy) * exp(-dy / 10.0)

2. REWARD SCALING: AZALT VEYA KALDIR
   - 0.1x → 0.5x veya 1.0x
   - Shaping signal'ların görünür olması için

3. DISTANCE PENALTY: AZALT
   - 0.05 → 0.02
   - Hız kontrolüne yer açmak için

4. HORIZONTAL VELOCITY: ARTIR
   - 0.03 → 0.05 veya 0.08
   - Drift'i azaltmak için

5. PROGRESSIVE REWARDS: İRTİFA BAZLI
   - Yüksek irtifa: Distance önemli
   - Orta irtifa: Distance + velocity önemli
   - Düşük irtifa: Velocity çok önemli

6. SUCCESS CRITERIA: SIKILAŞTIR (Gevşetme değil!)
   - vy <= 2.5 m/s (4.5 → 2.5)
   - v_h <= 2.0 m/s (3.0 → 2.0)
   - dist_h <= 3.0m (4.0 → 3.0)

7. TILT BALANCE: EŞİTLE
   - Penalty ve bonus eşit ağırlıkta olmalı

8. BONUS: YERE YAKLAŞMA
   - Dy azaldıkça küçük bonus (agent'ı inişe teşvik et)

================================================================================
YENİ REWARD FUNCTION ÖNERİSİ:
================================================================================

def compute_reward_done(self, states):
    # ... mevcut state parsing ...
    
    # === TERMINAL === (aynı kalabilir)
    if dy >= 75.0 and vy > 0.5:
        return -500.0, True
    if abs(dx) >= 25.0 or abs(dz) >= 25.0:
        return -500.0, True
    if up_y < 0.35:
        return -500.0, True
    if w_mag > 10.0:
        return -500.0, True
    
    # === LANDING CHECK === (sıkılaştırılmış)
    if dy <= 1.5:
        in_zone = (dist_h < 3.0)  # 4.0 → 3.0
        if not in_zone:
            return -350.0, True
        
        ok_vy = (abs(vy) <= 2.5)   # 4.5 → 2.5
        ok_vh = (v_h <= 2.0)       # 3.0 → 2.0
        ok_tilt = (up_y >= 0.85)
        ok_spin = (w_mag <= 4.0)
        
        if ok_vy and ok_vh and ok_tilt and ok_spin:
            bonus = (self.max_steps - self.step_count) * 0.1
            return 1500.0 + bonus, True
        else:
            return -300.0, True
    
    # === SHAPING === (YENİDEN TASARLANDI)
    reward = 0.0
    
    # 1. Step penalty (aynı)
    reward -= 0.02
    
    # 2. Distance penalty (azaltıldı: 0.05 → 0.02)
    reward -= 0.02 * dist_h
    
    # 3. Horizontal velocity (artırıldı: 0.03 → 0.06)
    reward -= 0.06 * v_h
    
    # 4. VERTICAL VELOCITY (HER İRTİFADA AKTİF, PROGRESSIVE)
    if vy < 0.0:  # Aşağı düşüyor
        # İrtifa azaldıkça artan penalty
        # dy=20m → ~0.5x, dy=5m → ~1.5x, dy=1.5m → ~2.0x
        altitude_factor = 1.0 + (15.0 / (dy + 1.0))
        reward -= 0.08 * abs(vy) * altitude_factor
    
    # 5. TILT (balanced)
    if up_y < 0.85:
        reward -= 0.08 * (0.85 - up_y)
    else:
        reward += 0.08 * (up_y - 0.85)  # Bonus eşit ağırlıkta
    
    # 6. Spin (aynı)
    reward -= 0.01 * w_mag
    
    # 7. YERE YAKLAŞMA BONUSU (yeni)
    # Dy azaldıkça küçük bonus (agent'ı inişe teşvik et)
    if dy < 20.0:
        approach_bonus = 0.05 * np.exp(-dy / 5.0)  # Max ~0.05
        reward += approach_bonus
    
    # 8. HIZ KONTROLÜ BONUSU (yeni)
    # Yavaş düşüyorsa bonus ver (vy > -2 m/s)
    if vy < 0.0 and abs(vy) < 2.0:
        slow_descent_bonus = 0.03 * (2.0 - abs(vy)) / 2.0  # Max ~0.03
        reward += slow_descent_bonus
    
    if self.step_count >= self.max_steps:
        return reward - 60.0, True
    
    return reward, False

# Reward scaling: 0.1 → 0.5 (daha görünür shaping)
reward_step *= 0.5  # 0.1 → 0.5

================================================================================
BEKLENEN ETKİLER:
================================================================================

1. Hız Kontrolü:
   - Agent yüksek irtifada bile hız kontrolü yapacak
   - Progressive penalty → agent erken fren yapacak
   - Slow descent bonus → yavaş iniş teşvik edilecek

2. Yatay Kontrol:
   - Horizontal velocity penalty artırıldı → drift azalacak
   - Distance penalty azaltıldı → hız kontrolüne yer açıldı

3. Shaping Signal:
   - Reward scaling 0.5x → shaping signal'lar görünür olacak
   - Approach bonus → agent inişe teşvik edilecek

4. Balance:
   - Tilt penalty/bonus eşit → daha dengeli
   - Multi-objective optimization daha iyi çalışacak

================================================================================
RİSKLER VE NOTLAR:
================================================================================

1. Reward scaling artırılırsa:
   - Value loss patlaması riski (ama 0.5x güvenli)
   - Learning rate ayarlaması gerekebilir

2. Success criteria sıkılaştırılırsa:
   - İlk başta başarı oranı daha da düşebilir
   - Curriculum learning gerekebilir (kolaydan zora)

3. Progressive velocity penalty:
   - Agent çok erken fren yapabilir (hover davranışı)
   - Altitude_factor'ü dengelemek gerekebilir

4. Test önerisi:
   - Önce reward scaling'i artır (0.1 → 0.5)
   - Sonra velocity penalty'yi ekle
   - Sonra diğer değişiklikleri yap

================================================================================

